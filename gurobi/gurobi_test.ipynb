{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import tomllib as tml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options looks like:\n",
    "```\n",
    "options = {\n",
    "    \"WLSACCESSID\": \"********-****-****-****-************\",\n",
    "    \"WLSSECRET\": \"********-****-****-****-************\",\n",
    "    \"LICENSEID\": _____,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get gurobi credentials\n",
    "options = tml.load(open(\"license.toml\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter WLSAccessID\n",
      "Set parameter WLSSecret\n",
      "Set parameter LicenseID to value 2527858\n",
      "Academic license 2527858 - for non-commercial use only - registered to mb___@ur.rochester.edu\n"
     ]
    }
   ],
   "source": [
    "# establish env (must close)\n",
    "env = gp.Env(params=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with MPS File\n",
    "I made a MPS file by solving LP.mod (written by Quan Luu) with GLPK for Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read MPS format model from file model.mps\n",
      "Reading time = 0.01 seconds\n",
      "LP: 757 rows, 729 columns, 2160 nonzeros\n",
      "Discarded solution information\n",
      "Gurobi Optimizer version 11.0.2 build v11.0.2rc0 (win64 - Windows 10.0 (19045.2))\n",
      "\n",
      "CPU model: 11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz, instruction set [SSE2|AVX|AVX2|AVX512]\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Academic license 2527858 - for non-commercial use only - registered to mb___@ur.rochester.edu\n",
      "Optimize a model with 757 rows, 729 columns and 2160 nonzeros\n",
      "Model fingerprint: 0xcc557f7f\n",
      "Variable types: 0 continuous, 729 integer (0 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [7e-03, 1e-01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 6e+00]\n",
      "Found heuristic solution: objective -0.0927000\n",
      "Presolve removed 27 rows and 0 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 730 rows, 729 columns, 2160 nonzeros\n",
      "Variable types: 0 continuous, 729 integer (729 binary)\n",
      "Found heuristic solution: objective -0.5618500\n",
      "\n",
      "Root relaxation: objective -9.707000e-01, 228 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0      -0.9707000   -0.97070  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (228 simplex iterations) in 0.02 seconds (0.01 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 3: -0.9707 -0.56185 -0.0927 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective -9.707000000000e-01, best bound -9.707000000000e-01, gap 0.0000%\n"
     ]
    }
   ],
   "source": [
    "m = gp.read(\"model.mps\", env=env)\n",
    "m.reset()\n",
    "m.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure this tells us much. Check `glpk_out.txt`, it has the full output of this solution. \n",
    "Notable slice:\n",
    "```\n",
    "730 rows, 729 columns, 2160 non-zeros\n",
    "      0: obj =  -4.657500000e-01 inf =   1.000e+01 (2)\n",
    "      5: obj =  -1.523000000e-01 inf =   0.000e+00 (0)\n",
    "*   224: obj =   6.790500000e-01 inf =   2.065e-14 (0) 1\n",
    "OPTIMAL LP SOLUTION FOUND\n",
    "Integer optimization begins...\n",
    "Long-step dual simplex will be used\n",
    "+   224: mip =     not found yet <=              +inf        (1; 0)\n",
    "+   224: >>>>>   6.790500000e-01 <=   6.790500000e-01   0.0% (1; 0)\n",
    "+   224: mip =   6.790500000e-01 <=     tree is empty   0.0% (0; 1)\n",
    "INTEGER OPTIMAL SOLUTION FOUND\n",
    "Time used:   0.0 secs\n",
    "Memory used: 1.9 Mb (1980226 bytes)\n",
    "STATES:\n",
    "[1 2 3]   [10 11 12]   [19 20 21]\n",
    "[4 5 6] , [13 14 15] , [22 23 24].\n",
    "[7 8 9]   [16 17 18]   [25 26 27]\n",
    "\n",
    "BUCKETS:\n",
    "Bucket 5: 1 2 3 4 5 6 7 8 9\n",
    "Bucket 11: 11 12 19 21\n",
    "Bucket 13: 10 13\n",
    "Bucket 14: 14 15\n",
    "Bucket 17: 16 17 18 25 26 27\n",
    "Bucket 23: 20 22 23 24\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup\n",
    "I'm going to try and convert this outright to a Gurobi model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish model (must close)\n",
    "model = gp.Model(env=env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the probability matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normpdf(x: float, mean: float, std: float) -> float:\n",
    "  var = float(std)**2\n",
    "  denom = (2*np.pi*var)**.5\n",
    "  num = np.exp(-(float(x)-float(mean))**2/(2*var))\n",
    "  return num/denom\n",
    "\n",
    "def gen_state_prob(num_traits: int, num_states: int):\n",
    "  mean = (num_states-1) / 2\n",
    "  std = mean / 1.25\n",
    "\n",
    "  state_prob = np.zeros(tuple([num_states] * num_traits), dtype=np.float64)\n",
    "  for inds in np.ndindex(state_prob.shape):\n",
    "    prob = 1\n",
    "    for ind in inds:\n",
    "      prob *= normpdf(ind, mean, std)\n",
    "    \n",
    "    state_prob[inds] = prob\n",
    "\n",
    "  state_prob = state_prob / np.sum(state_prob)\n",
    "\n",
    "  return state_prob.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the reward matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnumerize(num_traits: int, num_states: int, action: int):\n",
    "  ufaction = []\n",
    "  while action > 0:\n",
    "    ufaction.insert(0, action % num_states)\n",
    "    action = action // num_states\n",
    "\n",
    "  while len(ufaction) < num_traits:\n",
    "    ufaction.insert(0, 0)\n",
    "\n",
    "  return ufaction\n",
    "\n",
    "def reward_fn(param: tuple[float, float], state, action):\n",
    "  \n",
    "  l1dist = 0\n",
    "  for s, a in zip(state, action):\n",
    "    l1dist += abs(s - a)\n",
    "\n",
    "  return param[0] - param[1] * l1dist\n",
    "\n",
    "def reward_matrix(num_traits: int, num_states: int, reward_param: tuple[float, float]):\n",
    "  total_states = num_states**num_traits\n",
    "  res = np.array([[0 for _ in range(total_states)] for _ in range(total_states)], dtype=np.float64)\n",
    "  for x in range(total_states):\n",
    "    for y in range(total_states):\n",
    "      s1 = unnumerize(num_traits, num_states, x)\n",
    "      s2 = unnumerize(num_traits, num_states, y)\n",
    "\n",
    "      res[x, y] = reward_fn(reward_param, s1, s2)\n",
    "\n",
    "  return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_adj(num_traits: int, num_states: int, i: int, j: int):\n",
    "  s1 = unnumerize(num_traits, num_states, i)\n",
    "  s2 = unnumerize(num_traits, num_states, j)\n",
    "\n",
    "  l1dist = 0\n",
    "  for s, a in zip(s1, s2):\n",
    "    l1dist += abs(s - a)\n",
    "\n",
    "  return l1dist == 1\n",
    "\n",
    "def adj_matrix(num_traits: int, num_states: int):\n",
    "  total_states = num_states**num_traits\n",
    "  res = np.array([[0 for _ in range(total_states)] for _ in range(total_states)])\n",
    "  for x in range(total_states):\n",
    "    for y in range(total_states):\n",
    "      res[x, y] = 1 if is_adj(num_traits, num_states, x, y) else 0\n",
    "\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "t = 3\n",
    "n_per_t = 3\n",
    "n = n_per_t**t\n",
    "k = 6\n",
    "reward_param = (1, 0.5)\n",
    "\n",
    "V = np.asarray([i for i in range(n)])\n",
    "\n",
    "# state_prob = gen_state_prob(t, n_per_t)\n",
    "state_prob = np.full(27, 1/27, dtype=np.float64)\n",
    "\n",
    "reward = reward_matrix(t, n_per_t, reward_param)\n",
    "\n",
    "adj = adj_matrix(t, n_per_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hess Variables\n",
    "# GPL: var x{V, V} >= 0, <= 1, binary;\n",
    "x = model.addVars(V, V, ub=1, vtype=GRB.BINARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state objective\n",
    "# GPL: maximize EP: sum{i in V} PROB[i] * sum{j in V} x[i, j] * REWARD[i, j];\n",
    "# gp.quicksum( prob[i] * x[i][j] * reward[i][j] for i in V for j in V )\n",
    "objective = gp.quicksum( gp.quicksum( (state_prob[i] * x[i,j] * reward[i][j]) for j in V) for i in V )\n",
    "model.setObjective(objective, GRB.MAXIMIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter PoolSolutions to value 100000\n",
      "Set parameter PoolSearchMode to value 2\n"
     ]
    }
   ],
   "source": [
    "# add constraints\n",
    "\n",
    "# /* there are exactly k buckets */\n",
    "# kBucketConstr: sum{j in V} x[j, j] = k;\n",
    "k_bucket = gp.quicksum( (x[j,j]) for j in V ) == k\n",
    "model.addConstr(k_bucket)\n",
    "\n",
    "# /* a state can only belong to one bucket */\n",
    "# uniqueBucketConstr{i in V}: sum{j in V} x[i, j] = 1;\n",
    "unique_bucket = ( gp.quicksum( (x[i,j]) for j in V ) == 1 for i in V )\n",
    "model.addConstrs(unique_bucket)\n",
    "\n",
    "# /* a state cannot belong to a non-existant bucket */\n",
    "# nonexBucketConstr{i in V, j in V}: x[i, j] <= x[j, j];\n",
    "nonex_bucket = ( (x[i,j] <= x[j,j]) for i in V for j in V )\n",
    "model.addConstrs(nonex_bucket)\n",
    "\n",
    "# /* a state has to be adjacent to at least one other state in the bucket */\n",
    "# contiguousBucketConstr{i in V, j in V}: \n",
    "#   sum{l in V} x[i, j] * x[l, j] * ADJ[i, l] >= x[i, j];\n",
    "contiguous_bucket = ( gp.quicksum( (x[i, j] * x[l, j] * adj[i, l]) for l in V ) >= x[i, j] for i in V for j in V )\n",
    "model.addConstrs(contiguous_bucket)\n",
    "\n",
    "# multiple solutions\n",
    "model.Params.PoolSolutions = 100000\n",
    "model.Params.PoolSearchMode = 2\n",
    "\n",
    "model.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 11.0.2 build v11.0.2rc0 (win64 - Windows 10.0 (19045.2))\n",
      "\n",
      "CPU model: 11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz, instruction set [SSE2|AVX|AVX2|AVX512]\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Academic license 2527858 - for non-commercial use only - registered to mb___@ur.rochester.edu\n",
      "Optimize a model with 757 rows, 729 columns and 2160 nonzeros\n",
      "Model fingerprint: 0x0feebc51\n",
      "Model has 729 quadratic constraints\n",
      "Variable types: 0 continuous, 729 integer (729 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  QMatrix range    [1e+00, 1e+00]\n",
      "  QLMatrix range   [1e+00, 1e+00]\n",
      "  Objective range  [2e-02, 7e-02]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 6e+00]\n",
      "Presolve added 594 rows and 0 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 1351 rows, 729 columns, 5247 nonzeros\n",
      "Variable types: 0 continuous, 729 integer (729 binary)\n",
      "Found heuristic solution: objective -0.1296296\n",
      "\n",
      "Root relaxation: objective 6.111111e-01, 178 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0       0.6111111    0.61111  0.00%     -    0s\n",
      "\n",
      "Optimal solution found at node 0 - now completing solution pool...\n",
      "\n",
      "    Nodes    |    Current Node    |      Pool Obj. Bounds     |     Work\n",
      "             |                    |   Worst                   |\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0          -    0               -    0.61111      -     -    0s\n",
      "     0     0          -    0               -    0.61111      -     -    0s\n",
      "     0     2          -    0               -    0.61111      -     -    0s\n",
      " 38521 28230     cutoff   63               -    0.59259      -   1.7    5s\n",
      " 63456 40267          -   60               -    0.59259      -   1.5   10s\n",
      " 82303 48733          -   67               -    0.59259      -   1.4   15s\n",
      " 98285 55364          -   70               -    0.59259      -   1.3   20s\n",
      " 113314 61953     cutoff   79               -    0.59259      -   1.3   25s\n",
      " 123540 66383          -   63               -    0.59259      -   1.2   30s\n",
      " 136610 71577          -   77               -    0.59259      -   1.2   36s\n",
      " 144162 74473     cutoff   71               -    0.59259      -   1.2   40s\n",
      " 154096 78443     cutoff   76               -    0.59259      -   1.2   45s\n",
      " 162015 81456          -   72               -    0.59259      -   1.2   50s\n",
      " 168887 83923          -   81               -    0.59259      -   1.2   55s\n",
      " 175436 85582          -   81               -    0.59259      -   1.1   60s\n",
      " 183107 88189          -   84               -    0.59259      -   1.1   65s\n",
      " 188610 91320     cutoff   65               -    0.59259      -   1.1   70s\n",
      " 196275 94125          -   71               -    0.59259      -   1.1   77s\n",
      " 200012 94125          -   78               -    0.59259      -   1.1   80s\n",
      " 205085 96128          -   61               -    0.59259      -   1.1   85s\n",
      " 209392 99368          -   73               -    0.59259      -   1.1   90s\n",
      " 214353 101147          -   80               -    0.59259      -   1.1   95s\n",
      " 221273 99750          -   63         0.05556    0.59259   967%   1.1  102s\n",
      " 223539 101283          -   76         0.09259    0.59259   540%   1.1  105s\n",
      " 228961 102140          -   71         0.12963    0.59259   357%   1.1  110s\n",
      " 236234 103732          -   67         0.18519    0.59259   220%   1.1  118s\n",
      " 238247 103732          -   80         0.18519    0.59259   220%   1.1  120s\n",
      " 245690 106296     cutoff   80         0.20370    0.59259   191%   1.1  127s\n",
      " 249880 106303     cutoff   80         0.22222    0.59259   167%   1.1  131s\n",
      " 254306 105335     cutoff   63         0.24074    0.59259   146%   1.1  136s\n",
      " 258535 105875     cutoff   86         0.24074    0.59259   146%   1.1  140s\n",
      " 264293 107095          -   66         0.25926    0.59259   129%   1.1  146s\n",
      " 268410 106422          -   57         0.27778    0.59259   113%   1.1  150s\n",
      " 273120 107917     cutoff   74         0.27778    0.59259   113%   1.1  155s\n",
      " 278099 107436     cutoff   89         0.29630    0.59259   100%   1.1  160s\n",
      " 282477 108869          -   65         0.29630    0.59259   100%   1.0  165s\n",
      " 287056 107185          -   69         0.31481    0.59259  88.2%   1.0  170s\n",
      " 292472 108578          -   71         0.31481    0.59259  88.2%   1.0  175s\n",
      " 296297 110278     cutoff   79         0.31481    0.59259  88.2%   1.0  180s\n",
      " 300928 106465          -   84         0.33333    0.59259  77.8%   1.0  185s\n",
      " 306299 107783          -   74         0.33333    0.59259  77.8%   1.0  190s\n",
      " 310948 109233          -   81         0.33333    0.59259  77.8%   1.0  195s\n",
      " 317968 108298          -   75         0.35185    0.59259  68.4%   1.0  202s\n",
      " 322411 109685          -   74         0.35185    0.59259  68.4%   1.0  206s\n",
      " 326416 109685     cutoff   72         0.35185    0.59259  68.4%   1.0  210s\n",
      " 331202 111764     cutoff   81         0.35185    0.59259  68.4%   1.0  215s\n",
      " 336179 108212          -   71         0.37037    0.59259  60.0%   1.0  220s\n",
      " 341833 109774          -   75         0.37037    0.59259  60.0%   1.0  225s\n",
      " 347702 111262     cutoff   84         0.37037    0.59259  60.0%   1.0  230s\n",
      " 354186 108198          -   84         0.38889    0.59259  52.4%   1.0  236s\n",
      " 359529 109544          -   87         0.38889    0.59259  52.4%   1.0  240s\n",
      " 365973 111204          -   58         0.38889    0.59259  52.4%   1.0  246s\n",
      " 371185 113616          -   85         0.38889    0.59259  52.4%   1.0  251s\n",
      " 375149 114785     cutoff   70         0.38889    0.59259  52.4%   1.0  255s\n",
      " 382989 105851          -   80         0.40741    0.59259  45.5%   1.0  262s\n",
      " 388459 108648          -   58         0.40741    0.59259  45.5%   1.0  267s\n",
      " 392501 108648     cutoff   93         0.40741    0.59259  45.5%   1.0  270s\n",
      " 397446 111132     cutoff   87         0.40741    0.59259  45.5%   1.0  275s\n",
      " 402656 112678          -   83         0.40741    0.59259  45.5%   1.0  280s\n",
      " 409854 105461          -   71         0.42593    0.59259  39.1%   1.0  286s\n",
      " 414028 106580          -   80         0.42593    0.59259  39.1%   1.0  290s\n",
      " 418832 108043     cutoff   89         0.42593    0.59259  39.1%   1.0  295s\n",
      " 424437 109126          -   62         0.42593    0.59259  39.1%   1.0  300s\n",
      " 429424 110239          -   72         0.42593    0.59259  39.1%   1.0  305s\n",
      " 434890 111685     cutoff   91         0.42593    0.59259  39.1%   1.0  310s\n",
      " 440136 114165          -   69         0.42593    0.59259  39.1%   1.0  315s\n",
      " 444175 102263          -   67         0.44444    0.59259  33.3%   1.0  320s\n",
      " 449246 103216          -   79         0.44444    0.59259  33.3%   1.0  325s\n",
      " 454137 104418          -   60         0.44444    0.59259  33.3%   1.0  330s\n",
      " 460236 106669          -   65         0.44444    0.59259  33.3%   1.0  335s\n",
      " 465737 107659          -   78         0.44444    0.59259  33.3%   1.0  340s\n",
      " 471699 109091          -   65         0.44444    0.59259  33.3%   1.0  346s\n",
      " 474209 110268          -   76         0.44444    0.59259  33.3%   1.0  350s\n",
      " 477243 111510          -   73         0.44444    0.59259  33.3%   1.0  355s\n",
      " 481703 112554          -   72         0.44444    0.59259  33.3%   1.0  360s\n",
      " 486420 113959          -   55         0.44444    0.59259  33.3%   1.0  365s\n",
      " 493767 99581          -   38         0.46296    0.59259  28.0%   1.0  372s\n",
      " 497212 100533          -   73         0.46296    0.59259  28.0%   1.0  375s\n",
      " 504289 101971          -   76         0.46296    0.59259  28.0%   1.0  381s\n",
      " 509511 104166     cutoff   82         0.46296    0.59259  28.0%   1.0  386s\n",
      " 513551 105129          -   90         0.46296    0.59259  28.0%   1.0  390s\n",
      " 517255 105527          -   71         0.46296    0.59259  28.0%   1.0  395s\n",
      " 522033 107552          -   73         0.46296    0.59259  28.0%   1.0  400s\n",
      " 527226 108709          -   66         0.46296    0.59259  28.0%   1.0  405s\n",
      " 531776 109771          -   64         0.46296    0.59259  28.0%   1.0  410s\n",
      " 535766 111361     cutoff   79         0.46296    0.59259  28.0%   1.0  415s\n",
      " 540919 112649          -   83         0.46296    0.59259  28.0%   1.0  420s\n",
      " 547182 115014          -   63         0.46296    0.59259  28.0%   1.0  426s\n",
      " 551251 94473     cutoff   73         0.48148    0.59259  23.1%   1.0  430s\n",
      " 555733 96348     cutoff   71         0.48148    0.59259  23.1%   1.0  435s\n",
      " 562725 98379     cutoff   73         0.48148    0.59259  23.1%   1.0  441s\n",
      " 567372 99643     cutoff   67         0.48148    0.59259  23.1%   1.0  445s\n",
      " 572850 100724          -   68         0.48148    0.59259  23.1%   1.0  451s\n",
      " 578403 102797          -   64         0.48148    0.59259  23.1%   1.0  456s\n",
      " 582006 103104     cutoff   78         0.48148    0.59259  23.1%   1.0  460s\n",
      " 590598 106701          -   66         0.48148    0.59259  23.1%   1.0  467s\n",
      " 594375 107974          -   72         0.48148    0.59259  23.1%   1.0  470s\n",
      " 599636 109688          -   67         0.48148    0.59259  23.1%   1.0  475s\n",
      " 606659 111137     cutoff   80         0.48148    0.59259  23.1%   1.0  481s\n",
      " 611984 113945          -   65         0.48148    0.59259  23.1%   1.0  485s\n",
      " 617507 115823          -   63         0.48148    0.59259  23.1%   1.0  490s\n",
      " 622673 117008          -   77         0.48148    0.59259  23.1%   1.0  495s\n",
      " 629924 118269          -   85         0.48148    0.59259  23.1%   1.0  501s\n",
      " 634245 120032     cutoff   79         0.48148    0.59259  23.1%   1.0  505s\n",
      " 639550 122386          -   81         0.48148    0.59259  23.1%   1.0  510s\n",
      " 647852 124561          -   73         0.48148    0.59259  23.1%   1.0  518s\n",
      " 652087 82173     cutoff   79         0.50000    0.59259  18.5%   1.0  521s\n",
      " 656350 83782     cutoff   83         0.50000    0.59259  18.5%   1.0  525s\n",
      " 663057 85246          -   65         0.50000    0.59259  18.5%   1.0  531s\n",
      " 667446 85943          -   84         0.50000    0.59259  18.5%   1.0  535s\n",
      " 672745 87459          -   51         0.50000    0.59259  18.5%   1.0  540s\n",
      " 677988 88978          -   82         0.50000    0.59259  18.5%   1.0  545s\n",
      " 683602 91068          -   80         0.50000    0.59259  18.5%   1.0  550s\n",
      " 689921 93144          -   71         0.50000    0.59259  18.5%   1.1  555s\n",
      " 695765 94896     cutoff   73         0.50000    0.59259  18.5%   1.1  561s\n",
      " 701806 96076     cutoff   83         0.50000    0.59259  18.5%   1.1  566s\n",
      " 705841 97805          -   52         0.50000    0.59259  18.5%   1.1  570s\n",
      " 711319 99550     cutoff   75         0.50000    0.59259  18.5%   1.1  575s\n",
      " 717162 100556     cutoff   70         0.50000    0.59259  18.5%   1.1  581s\n",
      " 721282 102510          -   66         0.50000    0.59259  18.5%   1.1  585s\n",
      " 727353 104537     cutoff   92         0.50000    0.59259  18.5%   1.1  590s\n",
      " 733468 105553          -   61         0.50000    0.59259  18.5%   1.1  595s\n",
      " 739211 107658          -   35         0.50000    0.59259  18.5%   1.1  600s\n",
      " 745299 109567          -   54         0.50000    0.59259  18.5%   1.1  605s\n",
      " 753823 113412          -   79         0.50000    0.59259  18.5%   1.1  611s\n",
      " 759263 114455          -   74         0.50000    0.59259  18.5%   1.1  616s\n",
      " 763687 116437     cutoff   90         0.50000    0.59259  18.5%   1.1  620s\n",
      " 771386 118378     cutoff   77         0.50000    0.59259  18.5%   1.1  627s\n",
      " 774681 119093     cutoff   73         0.50000    0.59259  18.5%   1.1  630s\n",
      " 781085 120478     cutoff   75         0.50000    0.59259  18.5%   1.1  636s\n",
      " 785080 94577          -   74         0.51852    0.59259  14.3%   1.1  640s\n",
      " 790290 95657          -   70         0.51852    0.59259  14.3%   1.1  645s\n",
      " 795123 96830          -   63         0.51852    0.59259  14.3%   1.1  650s\n",
      " 801963 98993     cutoff   73         0.51852    0.59259  14.3%   1.1  655s\n",
      " 809252 100267          -   84         0.51852    0.59259  14.3%   1.1  661s\n",
      " 815069 102489          -   69         0.51852    0.59259  14.3%   1.1  665s\n",
      " 821686 104449     cutoff   71         0.51852    0.59259  14.3%   1.1  670s\n",
      " 826953 105480          -   72         0.51852    0.59259  14.3%   1.1  675s\n",
      " 831997 106630          -   81         0.51852    0.57407  10.7%   1.1  680s\n",
      " 839854 108299          -   67         0.51852    0.57407  10.7%   1.1  686s\n",
      " 844020 108958     cutoff   70         0.51852    0.57407  10.7%   1.1  690s\n",
      " 849550 109927     cutoff   68         0.51852    0.57407  10.7%   1.1  695s\n",
      " 857356 111515     cutoff   77         0.51852    0.57407  10.7%   1.1  701s\n",
      " 861706 112442          -   68         0.51852    0.57407  10.7%   1.1  705s\n",
      " 871214 114337          -   68         0.51852    0.57407  10.7%   1.1  712s\n",
      " 875409 115203          -   76         0.51852    0.57407  10.7%   1.1  716s\n",
      " 879801 116253     cutoff   86         0.51852    0.57407  10.7%   1.1  720s\n",
      " 888712 82997     cutoff   68         0.53704    0.57407  6.90%   1.1  726s\n",
      " 895674 84208          -   73         0.53704    0.57407  6.90%   1.1  731s\n",
      " 900519 84997          -   31         0.53704    0.57407  6.90%   1.1  735s\n",
      " 907802 86421          -   76         0.53704    0.57407  6.90%   1.1  740s\n",
      " 915138 87262          -   69         0.53704    0.57407  6.90%   1.1  746s\n",
      " 920118 88783          -   60         0.53704    0.57407  6.90%   1.1  751s\n",
      " 924228 89471          -   64         0.53704    0.57407  6.90%   1.1  755s\n",
      " 931212 90375          -   60         0.53704    0.57407  6.90%   1.1  760s\n",
      " 936406 91709          -   71         0.53704    0.57407  6.90%   1.1  765s\n",
      " 943547 93001     cutoff   69         0.53704    0.57407  6.90%   1.1  771s\n",
      " 948314 93826     cutoff   71         0.53704    0.57407  6.90%   1.1  775s\n",
      " 955722 94436     cutoff   74         0.53704    0.57407  6.90%   1.1  780s\n",
      " 963141 95646          -   84         0.53704    0.57407  6.90%   1.1  786s\n",
      " 967917 96573     cutoff   73         0.53704    0.57407  6.90%   1.1  790s\n",
      " 975196 97798          -   63         0.53704    0.57407  6.90%   1.1  795s\n",
      " 980636 98850          -   57         0.53704    0.57407  6.90%   1.1  800s\n",
      " 988409 100411          -   71         0.53704    0.57407  6.90%   1.1  805s\n",
      " 996754 101825          -   69         0.53704    0.57407  6.90%   1.1  811s\n",
      " 1002389 104141     cutoff   82         0.53704    0.57407  6.90%   1.1  815s\n",
      " 1011083 106572          -   87         0.53704    0.57407  6.90%   1.1  821s\n",
      " 1016969 107846          -   63         0.53704    0.57407  6.90%   1.1  825s\n",
      " 1025286 109722     cutoff   58         0.53704    0.57407  6.90%   1.1  830s\n",
      " 1033596 110575          -   63         0.53704    0.57407  6.90%   1.1  836s\n",
      " 1039003 112735     cutoff   73         0.53704    0.57407  6.90%   1.1  840s\n",
      " 1047669 114714     cutoff   76         0.53704    0.57407  6.90%   1.1  845s\n",
      " 1057403 116776     cutoff   87         0.53704    0.57407  6.90%   1.1  852s\n",
      " 1061521 117403     cutoff   82         0.53704    0.57407  6.90%   1.1  856s\n",
      " 1065730 118119          -   75         0.53704    0.57407  6.90%   1.1  860s\n",
      " 1074949 119725     cutoff   85         0.53704    0.57407  6.90%   1.1  867s\n",
      " 1078944 120476          -   43         0.53704    0.57407  6.90%   1.1  870s\n",
      " 1087195 121241          -   79         0.53704    0.57407  6.90%   1.1  875s\n",
      " 1096846 123891          -   82         0.53704    0.57407  6.90%   1.1  882s\n",
      " 1101076 124682          -   66         0.53704    0.57407  6.90%   1.1  885s\n",
      " 1109080 125722          -   75         0.53704    0.57407  6.90%   1.1  890s\n",
      " 1115418 127776          -   71         0.53704    0.57407  6.90%   1.1  895s\n",
      " 1124111 128980     cutoff   86         0.53704    0.57407  6.90%   1.1  900s\n",
      " 1129872 131025     cutoff   75         0.53704    0.57407  6.90%   1.1  905s\n",
      " 1138970 132220     cutoff   55         0.53704    0.57407  6.90%   1.1  911s\n",
      " 1144977 134200          -   64         0.53704    0.57407  6.90%   1.1  915s\n",
      " 1152468 135070          -   76         0.53704    0.57407  6.90%   1.1  921s\n",
      " 1159357 136196          -   73         0.53704    0.57407  6.90%   1.1  926s\n",
      " 1164557 137885     cutoff   77         0.53704    0.57407  6.90%   1.1  930s\n",
      " 1176754 47845          -   68         0.55556    0.57407  3.33%   1.1  935s\n",
      " 1189986 47419     cutoff   65         0.55556    0.57407  3.33%   1.1  940s\n",
      " 1204578 46695     cutoff   76         0.55556    0.57407  3.33%   1.1  945s\n",
      " 1220829 46420          -   70         0.55556    0.57407  3.33%   1.1  950s\n",
      " 1238423 46201          -   68         0.55556    0.57407  3.33%   1.1  955s\n",
      " 1255059 45757          -   66         0.55556    0.57407  3.33%   1.1  960s\n",
      " 1270895 45436          -   62         0.55556    0.57407  3.33%   1.1  965s\n",
      " 1285810 45083     cutoff   72         0.55556    0.57407  3.33%   1.1  970s\n",
      " 1303174 44355          -   70         0.55556    0.57407  3.33%   1.1  975s\n",
      " 1315783 44123     cutoff   71         0.55556    0.57407  3.33%   1.1  980s\n",
      " 1329842 43851          -   57         0.55556    0.57407  3.33%   1.0  985s\n",
      " 1341456 43801          -   66         0.55556    0.57407  3.33%   1.0  990s\n",
      " 1355485 43588     cutoff   80         0.55556    0.57407  3.33%   1.0  995s\n",
      " 1368476 43535     cutoff   87         0.55556    0.57407  3.33%   1.0 1000s\n",
      " 1380798 43415          -   66         0.55556    0.57407  3.33%   1.0 1005s\n",
      " 1394630 43181     cutoff   28         0.55556    0.57407  3.33%   1.0 1010s\n",
      " 1406945 43074     cutoff   52         0.55556    0.57407  3.33%   1.0 1015s\n",
      " 1419082 43000          -   77         0.55556    0.57407  3.33%   1.0 1020s\n",
      " 1433431 42813          -   62         0.55556    0.57407  3.33%   1.0 1025s\n",
      " 1444728 42611          -   76         0.55556    0.57407  3.33%   1.0 1030s\n",
      " 1458586 42085          -   55         0.55556    0.57407  3.33%   1.0 1035s\n",
      " 1473321 41206          -   48         0.55556    0.57407  3.33%   1.0 1040s\n",
      " 1488427 40624          -   79         0.55556    0.57407  3.33%   1.0 1045s\n",
      " 1504366 39792          -   52         0.55556    0.57407  3.33%   1.0 1050s\n",
      " 1520614 39430          -   76         0.55556    0.57407  3.33%   1.0 1055s\n",
      " 1533722 39029          -   73         0.55556    0.57407  3.33%   0.9 1060s\n",
      " 1548546 38748          -   81         0.55556    0.57407  3.33%   0.9 1065s\n",
      " 1561800 38325     cutoff   66         0.55556    0.57407  3.33%   0.9 1070s\n",
      " 1576151 37824          -   64         0.55556    0.57407  3.33%   0.9 1075s\n",
      " 1589211 37119          -   91         0.55556    0.57407  3.33%   0.9 1080s\n",
      " 1604452 36497     cutoff   56         0.55556    0.57407  3.33%   0.9 1085s\n",
      " 1618867 35997     cutoff   72         0.55556    0.57407  3.33%   0.9 1090s\n",
      " 1632361 35771          -   64         0.55556    0.57407  3.33%   0.9 1095s\n",
      " 1648691 35621          -   81         0.55556    0.57407  3.33%   0.9 1100s\n",
      " 1663099 35158     cutoff   87         0.55556    0.57407  3.33%   0.9 1105s\n",
      " 1677603 34720          -   40         0.55556    0.57407  3.33%   0.9 1110s\n",
      " 1691222 34556     cutoff   55         0.55556    0.57407  3.33%   0.9 1115s\n",
      " 1706286 33816     cutoff   80         0.55556    0.57407  3.33%   0.9 1120s\n",
      " 1720951 33251     cutoff   77         0.55556    0.57407  3.33%   0.9 1125s\n",
      " 1735630 32731          -   69         0.55556    0.57407  3.33%   0.9 1130s\n",
      " 1747539 32210     cutoff   68         0.55556    0.57407  3.33%   0.9 1135s\n",
      " 1762010 31525          -   78         0.55556    0.57407  3.33%   0.9 1140s\n",
      " 1775049 31241     cutoff   70         0.55556    0.57407  3.33%   0.8 1145s\n",
      " 1790433 31056          -   49         0.55556    0.57407  3.33%   0.8 1150s\n",
      " 1802910 30880          -   68         0.55556    0.57407  3.33%   0.8 1155s\n",
      " 1817135 30576     cutoff   60         0.55556    0.57407  3.33%   0.8 1160s\n",
      " 1830006 30385     cutoff   66         0.55556    0.57407  3.33%   0.8 1165s\n",
      " 1843860 29607     cutoff   69         0.55556    0.57407  3.33%   0.8 1170s\n",
      " 1857944 29471          -   58         0.55556    0.57407  3.33%   0.8 1175s\n",
      " 1870369 29309          -   50         0.55556    0.57407  3.33%   0.8 1180s\n",
      " 1883599 28950          -   64         0.55556    0.57407  3.33%   0.8 1185s\n",
      " 1898141 28650          -   65         0.55556    0.57407  3.33%   0.8 1190s\n",
      " 1911432 28457          -   68         0.55556    0.57407  3.33%   0.8 1195s\n",
      " 1923414 28105          -   36         0.55556    0.57407  3.33%   0.8 1200s\n",
      " 1934193 27591          -   54         0.55556    0.57407  3.33%   0.8 1205s\n",
      " 1948536 26948          -   47         0.55556    0.57407  3.33%   0.8 1210s\n",
      " 1959405 26728          -   63         0.55556    0.57407  3.33%   0.8 1215s\n",
      " 1973448 26140          -   71         0.55556    0.57407  3.33%   0.8 1220s\n",
      " 1989164 25367          -   65         0.55556    0.57407  3.33%   0.8 1225s\n",
      " 2002351 24862          -   53         0.55556    0.57407  3.33%   0.8 1230s\n",
      " 2015880 24409          -   73         0.55556    0.57407  3.33%   0.8 1235s\n",
      " 2031245 23884          -   82         0.55556    0.57407  3.33%   0.8 1240s\n",
      " 2046293 23569          -   73         0.55556    0.57407  3.33%   0.8 1245s\n",
      " 2058166 23091          -   76         0.55556    0.57407  3.33%   0.8 1250s\n",
      " 2073885 22291          -   78         0.55556    0.57407  3.33%   0.8 1255s\n",
      " 2089236 21553          -   62         0.55556    0.57407  3.33%   0.8 1260s\n",
      " 2104263 21132     cutoff   72         0.55556    0.57407  3.33%   0.8 1265s\n",
      " 2117310 20801          -   64         0.55556    0.57407  3.33%   0.8 1270s\n",
      " 2130383 20604          -   65         0.55556    0.57407  3.33%   0.8 1275s\n",
      " 2143450 20315     cutoff   81         0.55556    0.57407  3.33%   0.8 1280s\n",
      " 2156897 19705          -   65         0.55556    0.57407  3.33%   0.8 1285s\n",
      " 2170041 19024     cutoff   68         0.55556    0.57407  3.33%   0.8 1290s\n",
      " 2182792 18350          -   61         0.55556    0.57407  3.33%   0.8 1295s\n",
      " 2192640 18102          -   72         0.55556    0.57407  3.33%   0.8 1300s\n",
      " 2203673 17545          -   63         0.55556    0.57407  3.33%   0.8 1305s\n",
      " 2217265 17315          -   74         0.55556    0.57407  3.33%   0.8 1310s\n",
      " 2229375 16954          -   74         0.55556    0.57407  3.33%   0.8 1315s\n",
      " 2239938 16726     cutoff   81         0.55556    0.57407  3.33%   0.8 1320s\n",
      " 2251274 16521          -   72         0.55556    0.57407  3.33%   0.8 1325s\n",
      " 2263164 15978     cutoff   85         0.55556    0.57407  3.33%   0.8 1330s\n",
      " 2276838 15811          -   47         0.55556    0.57407  3.33%   0.8 1335s\n",
      " 2285616 15589          -   85         0.55556    0.57407  3.33%   0.8 1340s\n",
      " 2297219 15414          -   70         0.55556    0.57407  3.33%   0.8 1345s\n",
      " 2310460 14877     cutoff   76         0.55556    0.57407  3.33%   0.8 1350s\n",
      " 2320656 14226     cutoff   64         0.55556    0.57407  3.33%   0.8 1355s\n",
      " 2330558 13911     cutoff   73         0.55556    0.57407  3.33%   0.8 1360s\n",
      " 2343981 13336     cutoff   78         0.55556    0.57407  3.33%   0.8 1365s\n",
      " 2355765 12957          -   58         0.55556    0.57407  3.33%   0.8 1370s\n",
      " 2367690 12582          -   70         0.55556    0.57407  3.33%   0.8 1375s\n",
      " 2379255 12193          -   76         0.55556    0.57407  3.33%   0.8 1380s\n",
      " 2392818 11768     cutoff   80         0.55556    0.57407  3.33%   0.8 1385s\n",
      " 2405949 11439          -   73         0.55556    0.57407  3.33%   0.8 1390s\n",
      " 2417586 11175          -   71         0.55556    0.57407  3.33%   0.8 1395s\n",
      " 2428265 10982     cutoff   71         0.55556    0.57407  3.33%   0.8 1400s\n",
      " 2440107 10629     cutoff   72         0.55556    0.57407  3.33%   0.8 1405s\n",
      " 2452508 10128          -   75         0.55556    0.57407  3.33%   0.8 1410s\n",
      " 2464301  9705     cutoff   79         0.55556    0.57407  3.33%   0.8 1415s\n",
      " 2479132  9242     cutoff   79         0.55556    0.57407  3.33%   0.8 1420s\n",
      " 2489685  9027          -   64         0.55556    0.57407  3.33%   0.8 1425s\n",
      " 2501145  8767          -   63         0.55556    0.57407  3.33%   0.8 1430s\n",
      " 2512593  8629          -   62         0.55556    0.57407  3.33%   0.8 1436s\n",
      " 2523384  8370          -   71         0.55556    0.57407  3.33%   0.8 1441s\n",
      " 2531838  8242     cutoff   78         0.55556    0.57407  3.33%   0.8 1445s\n",
      " 2542780  7843          -   64         0.55556    0.57407  3.33%   0.8 1450s\n",
      " 2554783  7505          -   65         0.55556    0.57407  3.33%   0.8 1455s\n",
      " 2565906  7060     cutoff   85         0.55556    0.57407  3.33%   0.8 1460s\n",
      " 2577317  6660          -   70         0.55556    0.57407  3.33%   0.8 1465s\n",
      " 2587122  6405     cutoff   61         0.55556    0.57407  3.33%   0.8 1470s\n",
      " 2600763  5947          -   77         0.55556    0.57407  3.33%   0.8 1476s\n",
      " 2609569  5869          -   58         0.55556    0.57407  3.33%   0.8 1480s\n",
      " 2621269  5487     cutoff   56         0.55556    0.57407  3.33%   0.8 1485s\n",
      " 2631091  5329          -   74         0.55556    0.57407  3.33%   0.8 1490s\n",
      " 2639860  5325          -   69         0.55556    0.57407  3.33%   0.9 1495s\n",
      " 2650493  5014          -   72         0.55556    0.57407  3.33%   0.9 1500s\n",
      " 2658641  4746          -   69         0.55556    0.57407  3.33%   0.9 1505s\n",
      " 2669075  4459          -   81         0.55556    0.57407  3.33%   0.9 1510s\n",
      " 2683123  4192          -   87         0.55556    0.57407  3.33%   0.9 1515s\n",
      " 2693623  3836     cutoff   83         0.55556    0.57407  3.33%   0.9 1520s\n",
      "\n",
      "Explored 2703536 nodes (2331597 simplex iterations) in 1523.79 seconds (260.71 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 100000: 0.611111 0.611111 0.611111 ... 0.574074\n",
      "No other solutions better than 0.574074\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 6.111111111111e-01, best bound 6.111111111111e-01, gap 0.0000%\n"
     ]
    }
   ],
   "source": [
    "# can we solve?\n",
    "model.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Extraction\n",
    "This was a little easier than I thought, thanks to Quan's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 5: 2 4 5 6 8 \n",
      "Bucket 10: 1 10 13 19 \n",
      "Bucket 12: 3 11 12 15 21 \n",
      "Bucket 16: 7 16 25 \n",
      "Bucket 18: 9 17 18 27 \n",
      "Bucket 23: 14 20 22 23 24 26 \n"
     ]
    }
   ],
   "source": [
    "centers = [j for j in V if x[j,j].getAttr(\"x\") == 1]\n",
    "\n",
    "for j in centers:\n",
    "    print(f\"Bucket {j+1}: \", end=\"\")\n",
    "    members = [i for i in V if x[i,j].getAttr(\"x\") == 1]\n",
    "    for i in members:\n",
    "        print(f\"{i+1} \", end=\"\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ```\n",
    "STATES:\n",
    "[1 2 3]   [10 11 12]   [19 20 21]\n",
    "[4 5 6] , [13 14 15] , [22 23 24].\n",
    "[7 8 9]   [16 17 18]   [25 26 27]\n",
    "\n",
    "BUCKETS:\n",
    "Bucket 5: 1 2 3 4 5 6 7 8 9\n",
    "Bucket 11: 11 12 19 21\n",
    "Bucket 13: 10 13\n",
    "Bucket 14: 14 15\n",
    "Bucket 17: 16 17 18 25 26 27\n",
    "Bucket 23: 20 22 23 24\n",
    "```\n",
    "\n",
    "GLPK Output again for comparison. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert strategy to calculate information content measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(num_traits, num_states, num_signals, strat):\n",
    "  total_states = num_states**num_traits\n",
    "  small_val = 1e-7\n",
    "  large_val = 1 - (small_val * (num_signals-1))\n",
    "\n",
    "  signal_strat = np.full((num_signals, total_states), small_val, dtype=np.float64)\n",
    "\n",
    "  for i, bucket in enumerate(strat):\n",
    "    for state in bucket:\n",
    "      signal_strat[i, state] = large_val\n",
    "\n",
    "  return signal_strat\n",
    "\n",
    "def info_measure(num_traits, num_states, num_signals, signal_prob, weighted=True) -> float:\n",
    "    \"\"\"Calculates the information content of the signals\n",
    "\n",
    "    Args:\n",
    "      signal_prob (np.ndarray): the probabilities of the signals\n",
    "      weighted (boolean): weighted/unweighted options\n",
    "\n",
    "    Returns:\n",
    "      inf (float): total information content measure\n",
    "      inf_sigs (list): information content by signal\n",
    "      inf_states (list): information content by state\n",
    "    \"\"\"\n",
    "    total_states = num_states**num_traits\n",
    "    signal_prob = signal_prob.reshape(num_signals, total_states)\n",
    "\n",
    "    prob = np.zeros_like(signal_prob)\n",
    "    for i in range(num_signals):\n",
    "      for j in range(total_states):\n",
    "        prob[i, j] = signal_prob[i, j] * state_prob[j]\n",
    "    prob_sig = [np.sum(prob[i]) for i in range(num_signals)]\n",
    "    prob = (prob.T / np.sum(prob, axis=1)).T\n",
    "\n",
    "    inf = 0\n",
    "    inf_sigs = []\n",
    "    inf_states = []\n",
    "    for i in range(num_signals):\n",
    "      inf_sig = 0\n",
    "      inf_states.append([])\n",
    "      for j in range(total_states):\n",
    "        inf_state = prob[i, j] * np.log(prob[i, j]/state_prob[j])\n",
    "        inf_sig += inf_state\n",
    "        \n",
    "        if weighted:\n",
    "          inf_state = prob_sig[i] * inf_state\n",
    "\n",
    "        inf_states[i].append(inf_state)\n",
    "\n",
    "      if weighted:\n",
    "        inf_sig = prob_sig[i] * inf_sig\n",
    "\n",
    "      inf_sigs.append(inf_sig)\n",
    "      inf += inf_sig\n",
    "\n",
    "    new_size = [num_signals]\n",
    "    new_size.extend([num_states] * num_traits)\n",
    "    inf_states = np.resize(np.array(inf_states), tuple(new_size))\n",
    "\n",
    "    return inf, inf_sigs, inf_states\n",
    "\n",
    "def stats(inf, inf_sigstates):\n",
    "  inf_states = np.sum(inf_sigstates, axis=0)\n",
    "\n",
    "  print(f\"Info measure = {inf}\")\n",
    "  print(f\"Info measure by states:\")\n",
    "\n",
    "  for t1 in inf_states:\n",
    "    for t2 in t1:\n",
    "      for t3 in t2:\n",
    "        print(f\"{t3:.3f}\", end=\" \")\n",
    "      print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the average information content measure of all n results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of solutions: 100000\n",
      "UNWEIGHTED\n",
      "Info measure = 10.77021518231782\n",
      "Info measure by states:\n",
      "0.448 0.383 0.451 \n",
      "0.381 0.369 0.384 \n",
      "0.449 0.387 0.453 \n",
      "\n",
      "0.387 0.373 0.386 \n",
      "0.365 0.286 0.369 \n",
      "0.388 0.373 0.390 \n",
      "\n",
      "0.457 0.393 0.453 \n",
      "0.386 0.377 0.393 \n",
      "0.448 0.386 0.454 \n",
      "\n",
      "\n",
      "WEIGHTED\n",
      "Info measure = 1.7199932914883274\n",
      "Info measure by states:\n",
      "0.066 0.063 0.067 \n",
      "0.062 0.062 0.063 \n",
      "0.066 0.063 0.067 \n",
      "\n",
      "0.063 0.063 0.063 \n",
      "0.062 0.057 0.062 \n",
      "0.063 0.063 0.063 \n",
      "\n",
      "0.067 0.064 0.067 \n",
      "0.063 0.063 0.063 \n",
      "0.066 0.063 0.067 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# n solutions\n",
    "n_solutions = model.getAttr(\"SolCount\")\n",
    "print(f\"Number of solutions: {n_solutions}\")\n",
    "\n",
    "new_size = [k]\n",
    "new_size.extend([n_per_t] * t)\n",
    "\n",
    "total_info = 0\n",
    "total_info_sigstates = np.zeros(tuple(new_size))\n",
    "total_w_info = 0\n",
    "total_w_info_sigstates = np.zeros(tuple(new_size))\n",
    "\n",
    "for n in range(0, n_solutions):\n",
    "    model.params.SolutionNumber = n\n",
    "    # print(f\"Solution {n}\")\n",
    "    centers = [j for j in V if x[j,j].getAttr(\"Xn\") == 1]\n",
    "    strat = []\n",
    "    for j in centers:\n",
    "        # print(f\"Bucket {j+1}: \", end=\"\")\n",
    "        members = [i for i in V if x[i,j].getAttr(\"Xn\") == 1]\n",
    "        #     for i in members:\n",
    "        #         print(f\"{i+1} \", end=\"\")\n",
    "        #     print()\n",
    "        # print()\n",
    "        strat.append(members)\n",
    "\n",
    "    converted_strat = convert(t, n_per_t, k, strat)\n",
    "        \n",
    "    inf, inf_sigs, inf_sigstates = info_measure(t, n_per_t, k, converted_strat, False)\n",
    "\n",
    "    w_inf, w_inf_sigs, w_inf_sigstates = info_measure(t, n_per_t, k, converted_strat)\n",
    "\n",
    "    total_info += inf\n",
    "    total_info_sigstates += inf_sigstates\n",
    "\n",
    "    total_w_info += w_inf\n",
    "    total_w_info_sigstates += w_inf_sigstates\n",
    "    \n",
    "avg_info = total_info / n_solutions\n",
    "avg_info_sigstates = total_info_sigstates / n_solutions\n",
    "avg_w_info = total_w_info / n_solutions\n",
    "avg_w_info_sigstates = total_w_info_sigstates / n_solutions\n",
    "\n",
    "print(\"UNWEIGHTED\")\n",
    "stats(avg_info, avg_info_sigstates)\n",
    "print()\n",
    "print(\"WEIGHTED\")\n",
    "stats(avg_w_info, avg_w_info_sigstates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing these objects for best practice\n",
    "\n",
    "model.close()\n",
    "m.close()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
